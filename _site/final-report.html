<!DOCTYPE html>
<html lang="en-us">
    <head>
        <meta charset="UTF-8">
        <title>Cache Coherence Protocols Analyzer</title>
        <meta name="description" content="An analyzer for Cache Coherence Protocols under varying workloads.<br> Kshitiz Dange (kdange)  |  Yash Tibrewal (ytibrewa)"/>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#157878">
        <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
        <link rel="stylesheet" href="/assets/css/style.css?v=d675a24e4dae906ec96ad755b3351adf214e47f1">
    </head>
    <body>
        <section class="page-header">
            <h1 class="project-name">Cache Coherence Protocols Analyzer</h1>
            <h2 class="project-tagline">An analyzer for Cache Coherence Protocols under varying workloads.<br> Kshitiz Dange (kdange)  |  Yash Tibrewal (ytibrewa)</h2>
            <a href="https://kshitizdange.github.io/418CacheSim/images/Final_Presentation.pdf" class="btn">Presentation</a>
            <a href="proposal-page" class="btn">Project Proposal</a>
            <a href="checkpoint-page" class="btn">Checkpoint</a>
            <a href="final-report" class="btn">Final Report</a>
            <a href="authors-page" class="btn">Authors</a>
            
            <a href="http://github.com/kshitizdange/418CacheSim" class="btn">View on GitHub</a>
            
            <a href="http://15418.courses.cs.cmu.edu/spring2017/home" class="btn">15-418 Home</a>
        </section>

        <section class="main-content">
            
<h2 id="1-summary">1. SUMMARY</h2>
<blockquote>
  <p>We have implemented a Cache Simulator for analyzing how different Snooping-Based Cache Coherence Protocols - MSI, MESI,
MOSI, MOESI, Dragonfly, and Competitive Snooping; perform under various workloads. Given any program, we can use our
simulator to compare the performance of various protocols, based on number of Bus Transactions, Memory Requests,
Memory Write-Backs and Cache-to-Cache Transfers.</p>
</blockquote>

<h2 id="2-background">2. BACKGROUND</h2>
<blockquote>
  <p>We have studied about different snooping based Cache Coherence Protocols in class. Whenever a processor wants to read
or write something, it tries to use its own cache to avoid having to go to the memory each time (as it’s very slow).
But, when we have multiple processors, we need to synchronize the caches, so that all processors have a coherent view
of memory. For this, one approach is to use a snooping cache, where each cache monitors the memory reads and writes done
by other caches and takes some action based on those requests. MSI is one simple choice but there are other protocols
too which offer different kinds of benefits under specific workloads - MESI, MOSI, MOESI, Dragonfly, etc.
With this project we basically aim to study and demonstrate the advantages of each protocol over the others based on
some real-world, and some synthetic memory traces. We have also implemented a split-bus memory access, so that we can
mimic memory transactions similar to how they happen in real machines, instead of using an atomic bus. This gives us a
clear understanding of the performance of each snooping based protocol.
The inputs for analysis have been derived from programs that are concurrent in nature. We created two kinds of
programs - one based on locks, so that the read/writes are in a FIFO manner; and other based on random (aka wild)
access to the memory, which neither maintain consistency of data nor the order of transactions. We also designed some
artificial test inputs to be able to clearly differentiate the performance of each snooping based protocol.</p>
  <h2 id="21-snooping-based-cache-protocols">2.1. Snooping based cache protocols</h2>
  <p>The protocols that we have implemented are: <br />
Write-Invalidate Protocols <br />
1. MSI <br />
2. MESI <br />
3. MOSI <br />
4. MOESI <br /><br />
Write-Update Protocols <br />
1. Dragonfly (write-back update) <br /><br />
Hybrid Protocol <br />
1. Competitive Snooping(***) - While studying about various snooping based protocol, we found this hybrid protocol
intriguing. As it tries to combine the benefit of both write-invalidate and write-update protocols. It works by
maintaining a counter for each cache line. On a snooping update it’s decremented and on a processor access it’s
incremented. If on a snoop this counter reaches zero, instead of updating we invalidate the line. <br /><br />
<em>(***) This was not a part of our initial plan for the project, but since we are running ahead of our current schedule,
we will try our best to get this in before the final deadline. Some preliminary results have been updated now.</em> <br /><br />
<em>[!] UPDATE</em><br />
<em>We were able to implement this protocol in time, and have added the analysis in the results section of this report.</em>
<br /><br />
All of these protocols use write-allocate write-back caching. <br /><br />
Following are the <strong>State Transition Diagrams</strong> for above protocols. <br />
The basic MSI protocol with the Modified, Shared and Invalid states.
<img src="https://kshitizdange.github.io/418CacheSim/images/MSI.png" alt="alt text" title="MSI Protocol" /> <br />
MESI protocol adds ‘Exclusive’ state to MSI that reduces bus transactions caused by writes to cachelines that exist in a single cache.
<img src="https://kshitizdange.github.io/418CacheSim/images/MESI.png" alt="alt text" title="MESI Protocol" /> <br />
MOSI protocol adds ‘Owner’ state to MSI to reduce writebacks caused by reads from other processors.
<img src="https://kshitizdange.github.io/418CacheSim/images/MOSI.png" alt="alt text" title="MOSI Protocol" /> <br />
MOESI protocol combines the benefits of MESI and MOSI.
<img src="https://kshitizdange.github.io/418CacheSim/images/MOESI.png" alt="alt text" title="MOESI Protocol" /> <br />
Dragon protocol is a write-update protocol which on a write to cacheline, instead of invalidating the cacheline on other caches, sends an update message.
<img src="https://kshitizdange.github.io/418CacheSim/images/Dragon.png" alt="alt text" title="Dragon Protocol" /> <br /></p>
</blockquote>

<h2 id="3-approach">3. APPROACH:</h2>
<blockquote>
  <p>We use Intel’s pintool to instrument programs and generate memory traces for them. For this, we wrote our own pintool,
which for each instruction, checks whether it has a memory access or not. If it has a memory access, then at runtime we
record the following information: <br />
1. Whether the access is a read or a write <br />
2. The memory address for the load/store <br />
3. The thread-ID for the thread which performed the load/store <br /> <br />
This is saved in a file in the following format : <br /></p>
  <div class="language-javascript highlighter-rouge"><pre class="highlight"><code><span class="p">[</span><span class="nx">Thread</span><span class="o">-</span><span class="nx">ID</span><span class="p">]</span> <span class="p">[</span><span class="nx">R</span><span class="o">/</span><span class="nx">W</span><span class="p">]</span> <span class="p">[</span><span class="nx">Memory_Address</span><span class="p">]</span>
</code></pre>
  </div>
  <p>Example: <br /></p>
  <div class="language-javascript highlighter-rouge"><pre class="highlight"><code><span class="mi">0</span> <span class="nx">R</span> <span class="mh">0x1024</span>
<span class="mi">0</span> <span class="nx">W</span> <span class="mh">0x1024</span>
<span class="mi">1</span> <span class="nx">R</span> <span class="mh">0x1088</span>
<span class="mi">1</span> <span class="nx">W</span> <span class="mh">0x1088</span>
<span class="mi">2</span> <span class="nx">R</span> <span class="mh">0x1152</span>
</code></pre>
  </div>
  <p>For improved analysis of a program, we have written another pintool which allows demarcating the code which we want to
analyze using the simulator. This is to help in the scenario, where a program is very large and there are only specific
parts of the program that we want to analyze. We do this by calling two dummy functions: dummy_instr_start() and
dummy_instr_end(). The pintool recognizes calls to these functions, so when it sees a dummy_instr_start(), it
starts recording memory accesses and on dummy_instr_end(), it stops recording them. <br />
Once we have the memory trace generated by one of the pintools, we pass it to our Cache Simulator. <br /></p>
  <h3 id="32-cache-simulator-design">3.2. Cache Simulator Design:</h3>
  <p><img src="https://kshitizdange.github.io/418CacheSim/images/SystemArch.png" alt="alt text" title="System Architecture" /> <br />
Our Cache Simulator allows varying the following parameters: <br /></p>
  <div class="language-javascript highlighter-rouge"><pre class="highlight"><code><span class="o">-</span><span class="nx">c</span> <span class="nb">Number</span> <span class="nx">of</span> <span class="nx">Processor</span> <span class="nx">Cores</span><span class="o">/</span><span class="nx">Caches</span>
<span class="o">-</span><span class="nx">s</span> <span class="nx">Cache</span> <span class="nx">Size</span> <span class="p">(</span><span class="k">in</span> <span class="nx">MB</span><span class="p">)</span>
<span class="o">-</span><span class="nx">a</span> <span class="nx">Set</span> <span class="nx">Associativity</span>
<span class="o">-</span><span class="nx">p</span> <span class="nx">Cache</span> <span class="nx">Coherence</span> <span class="nx">Protocol</span> <span class="nx">to</span> <span class="nx">use</span> <span class="p">(</span><span class="nx">MSI</span><span class="o">/</span><span class="nx">MESI</span><span class="o">/</span><span class="nx">MOSI</span><span class="o">/</span><span class="nx">MOESI</span><span class="o">/</span><span class="nx">Dragon</span><span class="p">)</span>
<span class="o">-</span><span class="nx">t</span> <span class="nx">The</span> <span class="nx">trace</span> <span class="nx">file</span> <span class="nx">to</span> <span class="nx">use</span>
</code></pre>
  </div>
  <p>At initialization, we create caches for each processor, and we create two pThreads for each cache. One thread is
responsible for handling requests from the processor and initiating bus transactions if necessary (processor
transactions), while the other thread is responsible for responding to bus transactions (snooped bus transactions). For
accesses to memory, we have simulated a split-transaction bus. We create a pThread to act as the memory controller. We
have also added a small delay(of the order of couple ms), so that each memory request has some considerable latency
associated with it. This also means that while a memory request is pending, another cache might request the same
cacheline. This causes some interesting correctness issues that we will discuss later. <br /></p>
  <h4 id="the-high-level-workflow-of-our-simulator">The high level workflow of our simulator:</h4>
  <ul>
    <li>The main thread is responsible for reading the memory trace and instructing the appropriate caches to perform the
required memory read/write. This is implemented using a signalling mechanism (mutex and condition variables). <br /></li>
    <li>When a cache worker receives the signal for the memory access, it checks the state of the cacheline which has the
address and executes the protocol transitions if any. If the transition requires a snooping bus transaction, it
contends for the bus and broadcasts the action it wants to perform. The snooping threads for the other caches read
this action and depending on the protocol, they might change the status of the cache line, invalidate it, flush it to
memory, etc. If the cache worker who is performing the action does not have the cacheline in a valid state, one of the
other caches can place the data on the bus for this cache to read. Once the snooping is done, the cache worker might be
done with the handling of the memory access, or it might need to go to memory to fetch the cacheline (if no other cache
had the cacheline). In this case, we use the split transaction bus to request the cacheline from memory. <br /></li>
    <li>This continues till all caches are done with the memory accesses they were required to perform. <br />
Based on the actions that a cache performed, we update the number of snooping bus transactions, memory requests,
memory writebacks and cache to cache transfers. <br /></li>
  </ul>
</blockquote>

<h3 id="33-metrics-used-for-comparison">3.3. Metrics Used for Comparison</h3>
<blockquote>
  <ol>
    <li>Number of <strong>Snooping Bus Transactions</strong> - The number of bus transactions that the protocol implementation requires
for the program. <br /></li>
    <li>Number of <strong>Memory Requests</strong> - The number of cacheline loads that have to be served by main memory (and not another
cache).  <br /></li>
    <li>Number of <strong>Memory Write-Backs</strong> - The number of times any cacheline has to be written back to main memory. <br /></li>
    <li>Number of <strong>Cache to Cache Transfers</strong> - The number of cacheline loads that are served by another cache. <br />
 <br /> <br />
Ideally, we want as few bus transactions, memory requests, memory write-backs, or cache to cache transfers as possible
because all of them will have some amount of latency associated with them. If it is a choice between a memory request
and a cache to cache transfer, we always prefer the latter, because a memory request will have much higher latency
compared to getting data from another cache. <br /></li>
  </ol>
</blockquote>

<h3 id="34-interesting-synchronization-and-implementation-issues">3.4. Interesting Synchronization and Implementation Issues:</h3>
<blockquote>
  <p>As described earlier, we create separate threads to simulate various parts of the cache. To maintain and ensure
correctness, there are certain issues we need to take care of:</p>
  <h4 id="341-split-transaction-bus">3.4.1. Split Transaction Bus</h4>
  <p>Having a split transaction bus means that when there is a request from one cache pending, another cache can request
load of the same cacheline. In this case, the memory does not need to service the same request twice. It just needs to
provide the data for the cacheline to both the caches, or in other words, both the caches need to be listening for the
memory’s servicing of the same request. For implementing this, we create a separate condition variable for each
request. When a cache needs something serviced from memory, it first checks whether the access already exists in the
request table. If it already exists, instead of creating another memory request in the table, it simply waits on the
condition variable of the existing request. When the memory worker is done servicing the request, it performs a
broadcast on the condition variable associated with the request, so that all caches that were waiting for that request
can continue with their work.</p>
  <h4 id="342-cache-worker-and-snooping-worker">3.4.2. Cache Worker and Snooping Worker</h4>
  <p>We have two workers (pThreads) for each cache. One for servicing load/store requests from the processor and one for
servicing for snooping bus transactions. Both these threads require concurrent access to the same cache, and hence the
accessed need to be synchronized. Our implementation uses a single copy of the cache with pthread mutexes for
synchronization.</p>
  <h4 id="343-snooping-bus">3.4.3. Snooping Bus</h4>
  <p>The current implementation of the snooping bus uses pthread mutices and condition variables. The order in which caches
gain access to the bus is completely dependent on the implementation of pthread mutex and condition variables. Based
on our implementation, we can easily enforce a specific ordering on the arbitration, but we decided to use the simple
default implementation. To explain in brief, we have a mutex for getting access to the snooping bus for broadcasting
actions. Another mutex is required for other snooping caches to respond to this action. This also involves the
snooping caches informing the cache worker whether the cacheline would be in the exclusive or shared state, or if the
cache worker does not have the cacheline, whether the snooping cache can provide the data for the cacheline. A snooping
caches can also NACK the transaction, if its own cache has a conflicting pending transaction that is waiting to be
serviced by the split transaction bus. <br /></p>
</blockquote>

<h2 id="4-protocols-low-level-design-choices">4. PROTOCOLS’ LOW LEVEL DESIGN CHOICES:</h2>
<blockquote>
  <ul>
    <li>We have designed all the aforementioned protocols so that a cache miss will try to be serviced from another cache
(if a cache already has the data). This reduces the number of memory requests required. <br />
In MOSI and MOESI implementations, memory writebacks only happen when a cacheline in Modified or Owner state is
evicted. If a cacheline is invalidated due to the protocol, the cache transfers the cacheline to the new owner of the
cacheline. (We deem the cache which has a line in the Modified or Owner state to be the owner.) This enhances the
effect of the additional Owner state. <br /></li>
    <li>In our Dragon implementation, when a line is in the SharedClean or SharedModified state and tries to perform a write,
we first check whether it is the only cache holding the line or not. If it is the only one holding the cacheline, then
instead of having to perform a BusUpdate and moving to the SharedModified state, it can simply move to the Modified
state. This helps in the case where other caches have evicted that line causing only one cache to have the line. This
saves on unnecessary bus transactions <br /></li>
    <li>In Dragon write-back update, similar to MOSI and MOESI, a snooping bus transaction does not cause a memory writeback.
Only an eviction of a cacheline in a Modified or SharedModified state would cause a memory writeback. <br />
      <h3 id="nack">NACK</h3>
    </li>
    <li>As mentioned earlier, our choice of implementation for the snooping bus and the split transaction bus can
cause a snooping bus request to conflict with a pending split transaction bus. For example, while a memory load for a
cacheline is pending in the request table from a read issued by one processor, another cache can be requesting
exclusive access to the same cacheline on the snooping bus. In this case, if the cache with the pending request allows
the new request to go ahead, it will have to retry its memory load at a later time. This can cause a livelock, if the
two caches keep retrying and giving way to each other. To avoid this, if a snooping cache detects a conflicting action
on the snooping bus, it NACKs the action, which causes the cache worker to retry the transaction at a later time. This
avoids the livelock issue and ensures completion.</li>
  </ul>
</blockquote>

<h2 id="5-test-harness">5. TEST HARNESS</h2>
<blockquote>
  <p>To test if our cache implementation is working properly, we designed some artificial traces. For these traces, we already
knew the values of the metrics that would be generated by running our program. So, this made it possible to check that the our
cache implementation is working as expected. The scripts to generate these traces can be found in the “../418CacheSim/traces/”
directory. The scripts are:</p>
  <div class="language-javascript highlighter-rouge"><pre class="highlight"><code><span class="nx">MSIvMESI</span><span class="p">.</span><span class="nx">py</span>
<span class="nx">MSIvMOSI</span><span class="p">.</span><span class="nx">py</span>
<span class="nx">MOESIvsDragon</span><span class="p">.</span><span class="nx">py</span>
</code></pre>
  </div>
  <p>For benchmarking and comparisons between the protocols, we wrote a few test programs - wild_add, lock_add, wild_fill_bucket,
lock_fill_bucket. We also used our code from 15-618 assignments :- assignment 1 (mandelbrot) and assignment 3
(pagerank and bfs topdown). Additionally, we used tests from the Splash 2 application suite. Except for Splash 2, all tests were run on ghc-42 with 16 threads. The caches were setup to have a size of 1MB with a set-associativity of 8. As for Splash2, it was run with 4 threads on our own machines. (The Splash2 tests required additional programs which were not available on GHC machines). The program sizes were purposely chosen to be small so that the memory traces do not get too large. The results from the tests are shown below.</p>
</blockquote>

<h2 id="6-graphs-and-analysis">6. GRAPHS AND ANALYSIS</h2>
<blockquote>
  <ol>
    <li><strong>MSI vs MESI</strong>: This tests out an artificial trace that Read and Writes to unique addresses on separate cache
lines. This means that there is no requirement of sending out write-invalidation or write-update messages. From the
results, we can see that MSI and MOSI perform worse compared to others because they do not have an Exclusive state.
This means that on a write, they need to send out unnecessary write-invalidate messages on the bus. (All memory
writebacks here are due to cache evictions.) <br />
<img src="https://kshitizdange.github.io/418CacheSim/images/MSIvMESI.png" alt="alt text" title="MSIvMESI Stats Graph" /> <br /></li>
    <li><strong>MSI vs MOSI</strong>: This test has 16 threads where all of them are reading and writing to the same address. In a
normal MSI or MESI protocol, this would require writebacks on each invalidation. MOSI and MOESI on the other hand,
just get the cache to transfer the data to the new owner, avoiding the need for memory writebacks. <br />
<img src="https://kshitizdange.github.io/418CacheSim/images/MSIvMOSI.png" alt="alt text" title="MSIvMOSI Stats Graph" /> <br /></li>
    <li><strong>MOESI vs Dragon</strong>: This test has 16 threads, with one of them constantly writing to an address, while the others
keep reading from the same address. Write-invalidate protocols would require invalidations on each write. The Dragon
write-update protocol instead of invalidation sends an update to the other caches, so that they do no need get a cache
miss on each and every read. The results clearly show the low number of bus transactions required by Dragon. <br />
<img src="https://kshitizdange.github.io/418CacheSim/images/MOESIvDragon.png" alt="alt text" title="MOESIvDragon Stats Graph" /> <br /></li>
    <li><strong>Lock Add</strong>: In this test all the threads try to add ‘1’ to a global counter using locks. We see lower number of
memory writebacks in MOSI and MOESI because of the presence of the owner state. <br />
<img src="https://kshitizdange.github.io/418CacheSim/images/lock-add.png" alt="alt text" title="lock-Add" /> <br /></li>
    <li><strong>Wild Add</strong>: In this test all the threads try to add ‘1’ to a global counter without using locks. Here, the
exclusive state seems to be helping. Probably the read on the global variable and subsequent write to it brings out
the benefit in MESI and MOESI protocols. <br />
<img src="https://kshitizdange.github.io/418CacheSim/images/wild-add.png" alt="alt text" title="wild-Add" /> <br /></li>
    <li><strong>Lock Fill Bucket</strong>: This tests makes buckets for numbers, so that it can count how many times each number is
present in the array. This is done using locks. The dragon protocol is performing much worse here compared to others.
This is probably because updates to the buckets are not always needed by other processors, hence the updates on the
writes do not help. <br />
<img src="https://kshitizdange.github.io/418CacheSim/images/lock-fill.png" alt="alt text" title="lock-fill" /> <br /></li>
    <li><strong>Wild Fill Bucket</strong>: This tests makes buckets for numbers, so that it can count how many times each number is
present in the array. Locks are not used in this case. The results are similar to the lock fill bucket case. <br />
<img src="https://kshitizdange.github.io/418CacheSim/images/wild-fill.png" alt="alt text" title="wild-fill" /> <br /></li>
    <li><strong>Mandelbrot</strong>: This is the mandelbrot from Assignment 1. <br />
<img src="https://kshitizdange.github.io/418CacheSim/images/mandelbrot.png" alt="alt text" title="mandelbrot" /> <br /></li>
    <li><strong>Pagerank</strong>: This is the Pagerank from Assignment 3, run on tiny.graph. The score updates in one iteration are
read in the next iteration. This is probably also the reason why Dragon protocol is showing good results compared to
the write-invalidate protocols. <br />
<img src="https://kshitizdange.github.io/418CacheSim/images/pagerank.png" alt="alt text" title="pagerank" /> <br /></li>
    <li><strong>BFS</strong>: This is the BFS top-down approach from from Assignment 3 run on grid_100x100. <br />
<img src="https://kshitizdange.github.io/418CacheSim/images/bfs.png" alt="alt text" title="bfs" /> <br /></li>
    <li><strong>Ocean</strong>: This is the splash-2 ocean run with 4 processors and 18x18 grid. Write-update protocols seem to
perform noticeably better. <br />
<img src="https://kshitizdange.github.io/418CacheSim/images/ocean.png" alt="alt text" title="ocean" /> <br /></li>
  </ol>

  <h3 id="competitive-snooping">Competitive Snooping</h3>
  <p>This is a hybrid kind of protocol, a cross between write-invalidate and write-update. We take Dragon as a base and
associate an update counter with each cacheline. When, a processor accesses a cacheline, the counter is incremented
and when it receives an udpate from another processor, the counter is decremented. When, the counter reaches zero,
instead of updating the cacheline, we invalidate it. This basically means that the processor does not access the
cacheline very often and instead of updating it, invalidating it might be better. <br />
<strong>Result</strong> From our experiments, we did not see any noticeable improvements compared to others. The numbers were similar to
Dragon.</p>
</blockquote>

<h2 id="7-platform-choice">7. PLATFORM CHOICE</h2>
<blockquote>
  <p>We used C++ as the programming language for creating both the pintools and the cache simulator.</p>
</blockquote>

<h2 id="8-results">8. RESULTS</h2>
<blockquote>
  <h3 id="interesting-conclusions">Interesting Conclusions</h3>
  <h4 id="adding-e-state-">Adding ‘E’ state: <br /></h4>
  <p>To measure the performance differences caused by adding the Exclusive state to the protocols, we can look at the
differences in metrics in MSI vs MESI and MOSI vs MOESI. The main benefit of the Exclusive state is in reducing the
number of snooping bus transactions required. If we consider a parallel program where each thread works on a chunk of
an array and updates only that chunk, or if we assume a sequential program that has a single thread, then in these
cases, there will be a lot of cases where a program is reading a cacheline and updating it. In MSI, this would
translate to first loading the cacheline using a BusRd moving to the S state, and then performing a BusRdX and moving
to the M state. This requires two snooping bus transactions. In the case of MESI, this can be done in a single
transaction. The cache would perform a BusRd moving to the E state and then since no other cache has the cacheline,
there is no need of a BusRdX transaction to move to the M state. It can just silently change the status of the
cacheline to Modified. <br />
This gives a significant boost in programs which access and modify unique memory addresses. <br />
From the tests and memory traces that we generated, adding the Exclusive state reduced the number of snooping bus
transactions by 5-6%. (These tests also include other memory accesses, so this is a considerable reduction). <br /></p>
  <h4 id="adding-o-state--">Adding ‘O’ state : <br /></h4>
  <p>To measure the performance differences caused by adding the Owner state to the protocols, we can look at the
differences in metrics in MSI vs MOSI and MESI vs MOESI.
The intention in adding the Owner state is to reduce the number of memory write backs required by the protocol
especially on cache invalidations. To understand how the Owner state helps, we can imagine a parallel program where
one thread writes to a memory address, while the other threads simply observe or read the value at the memory address.
In the MSI protocol case, whenever the thread writes to the memory address, it would move to the Modified state and
when the other threads would want to read from that address, it would have to flush the data to memory and move to
the Shared state. This would result in a memory writeback each time. On the other hand, if we consider the MOSI
protocol, instead of having to flush the contents to memory each time, the writer thread would move to the Owner
state instead of invalidating the cacheline. To ensure correctness, whenever a cacheline in the Owner or Modified
state is evicted from the cache, the memory writeback needs to be performed. <br />
From the tests and memory traces that we generated, adding the Exclusive state reduced the number of snooping bus
transactions by 8-10%. (Again, these tests also include other memory accesses, so this is a considerable reduction) <br /></p>
  <h4 id="write-invalidation-vs-write-update-">Write-Invalidation vs Write-Update <br /></h4>
  <p>Since we have implemented both write invalidation and write update protocols, our simulator can also tell whether
for a given program or memory trace, write invalidation protocols will be better or write update. <br />
For a write-invalidation protocol, when a processor writes to a memory location, other processor caches need to
invalidate that cacheline. In a write-update protocol, instead of invalidating the cachelines, it sends the updated
cacheline to the other caches. Therefore, in cases where the other processors will need to read those values in the
future, write-update performs well, but if the other processors are not going to be needing those values, then the
updates are not going to be of any use, and will just cause extra bus transactions. Therefore, the effects of the
protocol would be completely dependent. <br />
From our tests, we saw lesser number of bus transactions with Dragon for page_rank, mandelbrot and Splash2-Ocean. In
pagerank, the score updated in one iteration is used by the other threads in the next iteration. This would explain
why updating rather than invalidating reduced the number of bus transactions. <br /></p>
</blockquote>

<h2 id="9-references">9. REFERENCES</h2>
<blockquote>
  <ol>
    <li>http://wiki.expertiza.ncsu.edu/index.php/CSC/ECE_506_Spring_2010/8a_sk?ref=driverlayer.com/image</li>
    <li>Suleman, Linda Bigelow Veynu Narasiman Aater. “An Evaluation of Snoop-Based Cache Coherence Protocols.”</li>
    <li>en.wikipedia.org</li>
    <li>15-618 Course Slides</li>
    <li>https://software.intel.com/en-us/articles/pin-a-dynamic-binary-instrumentation-tool</li>
    <li>https://software.intel.com/sites/landingpage/pintool/docs/65163/Pin/html/index.html</li>
    <li>Grahn, Håkan, Per Stenström, and Michel Dubois. “Implementation and evaluation of update-based cache protocols under relaxed memory consistency models.” Future Generation Computer Systems 11.3 (1995): 247-271.</li>
  </ol>
</blockquote>

<h2 id="10-list-of-work-by-each-student">10. LIST OF WORK BY EACH STUDENT</h2>
<blockquote>
  <h3 id="equal-work-was-performed-by-both-project-members">Equal work was performed by both project members.</h3>
</blockquote>



            <footer class="site-footer">
                
                <span class="site-footer-owner"><a href="http://github.com/kshitizdange/418CacheSim">418CacheSim</a>
                            is maintained by <a href="mailto:kdange@cmu.edu">Kshitiz Dange </a> and
                            <a href="mailto:ytibrewa@andrew.cmu.edu">Yash Tibrewal </a></span>
                
                <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
            </footer>
        </section>

        
    </body>
</html>
